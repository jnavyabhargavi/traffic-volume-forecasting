{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aggregated & Preprocessed Traffic Data \u2014 EDA, Cleaning & Feature Engineering\n",
        "\n",
        "**What this notebook does (end-to-end)**\n",
        "\n",
        "1. Loads raw traffic data (CSV(s)) and optional events/metadata files.  \n",
        "2. Cleans data: handles missing values, duplicates, and corrects dtypes.  \n",
        "3. Aggregates traffic counts to hourly intervals per junction.  \n",
        "4. Preprocesses numeric columns (scaling/normalizing) and encodes categorical/time features.  \n",
        "5. Engineers time-based features, lag features, rolling windows, weekend/event flags.  \n",
        "6. Performs feature relevance analysis (correlation, RandomForest importance).  \n",
        "7. Saves the final aggregated & preprocessed dataset for modeling/submission.\n",
        "\n",
        "**How to use**\n",
        "- Place your raw CSV(s) in a folder `data/`. Typical expected columns in traffic CSV:\n",
        "  - `timestamp` (or `datetime`) \u2014 string parsable by `pd.to_datetime`\n",
        "  - `junction_id` (or similar) \u2014 identifier for junction/intersection\n",
        "  - `vehicle_count` (or `count`, `vehicles`) \u2014 numeric count\n",
        "  - optionally `vehicle_type`, `lane_id`, etc.\n",
        "\n",
        "- If you have a special events file, name it `events.csv` with at least:\n",
        "  - `date` or `start_date`/`end_date` and `event_name` or `is_event` flag.\n",
        "\n",
        "- Run the notebook, inspect outputs, adjust column names in the \"CONFIG\" cell to match your dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "\n",
        "This notebook uses common Python data libraries:\n",
        "- pandas, numpy, matplotlib, scikit-learn\n",
        "- (optional) statsmodels for time series diagnostics, but not required\n",
        "\n",
        "Make sure these libraries are installed in your environment. The notebook is written to be robust if a few optional packages are missing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# CONFIG: Edit these to match your files/columns\n",
        "# -----------------------\n",
        "RAW_DATA_DIR = \"data\"                      # folder containing raw CSVs\n",
        "RAW_FILES = [\"traffic_raw.csv\"]            # list of CSV filenames to load (order doesn't matter)\n",
        "EVENTS_FILE = \"events.csv\"                 # optional special events file (set to None if not available)\n",
        "\n",
        "# Column name mapping: rename your dataset's columns to these canonical names after load\n",
        "COL_TIMESTAMP = \"timestamp\"      # e.g. \"timestamp\" or \"datetime\"\n",
        "COL_JUNCTION = \"junction_id\"     # identifier for junction/intersection\n",
        "COL_COUNT = \"vehicle_count\"      # numeric vehicle count column name\n",
        "\n",
        "# Output filenames\n",
        "AGG_OUT_CSV = \"aggregated_preprocessed.csv\"\n",
        "\n",
        "# Aggregation settings\n",
        "RESAMPLE_FREQ = \"H\"   # hourly aggregation\n",
        "TIMEZONE = None       # e.g. \"Asia/Kolkata\" if timestamps are localized; set to None if naive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "from datetime import timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# plt style: we will use matplotlib only (no seaborn) and won't force specific colors.\n",
        "# For notebooks, enable inline plotting (if using jupyter).\n",
        "try:\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions reused throughout the notebook\n",
        "\n",
        "def load_and_concat_csvs(folder, file_list):\n",
        "    \"\"\"Load CSVs specified in file_list located in folder, concat into one DataFrame.\"\"\"\n",
        "    dfs = []\n",
        "    for fn in file_list:\n",
        "        path = os.path.join(folder, fn)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: {path} not found \u2014 skipping.\")\n",
        "            continue\n",
        "        df = pd.read_csv(path)\n",
        "        df['_source_file'] = fn\n",
        "        dfs.append(df)\n",
        "    if len(dfs) == 0:\n",
        "        raise FileNotFoundError(\"No data files found. Put your CSV(s) in the data/ directory and update RAW_FILES.\")\n",
        "    combined = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "    return combined\n",
        "\n",
        "def safe_to_datetime(df, col, tz=None):\n",
        "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "    if tz is not None:\n",
        "        # Convert naive timestamps to the desired timezone (assumes input is naive/local)\n",
        "        df[col] = df[col].dt.tz_localize(tz, ambiguous='infer', nonexistent='shift_forward')\n",
        "    return df\n",
        "\n",
        "def print_df_info(df, name='df'):\n",
        "    print(f\"=== {name} shape: {df.shape} ===\")\n",
        "    print(df.dtypes)\n",
        "    print(df.head(3))\n",
        "    print('\\nMissing values per column:\\n', df.isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "raw = load_and_concat_csvs(RAW_DATA_DIR, RAW_FILES)\n",
        "print('Loaded rows:', len(raw))\n",
        "print_df_info(raw, 'raw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize column names\n",
        "raw_cols = {c.lower(): c for c in raw.columns}  # quick mapping\n",
        "# Rename to canonical names if possible (reasonable heuristics)\n",
        "rename_map = {}\n",
        "for c in raw.columns:\n",
        "    lc = c.lower()\n",
        "    if 'time' in lc or 'date' in lc:\n",
        "        rename_map[c] = COL_TIMESTAMP\n",
        "    elif 'junction' in lc or 'intersection' in lc:\n",
        "        rename_map[c] = COL_JUNCTION\n",
        "    elif 'count' in lc or 'vehicle' in lc or 'vehicles' in lc:\n",
        "        # prefer a name containing 'count'\n",
        "        rename_map[c] = COL_COUNT\n",
        "\n",
        "if rename_map:\n",
        "    raw = raw.rename(columns=rename_map)\n",
        "    print('Columns renamed using heuristics:', rename_map)\n",
        "else:\n",
        "    print('No heuristic renames applied; ensure your columns match the CONFIG variables.')\n",
        "\n",
        "# Make sure canonical columns are present\n",
        "missing = [COL_TIMESTAMP, COL_JUNCTION, COL_COUNT]\n",
        "for m in missing:\n",
        "    if m not in raw.columns:\n",
        "        print(f\"WARNING: expected column '{m}' not found. Please update CONFIG or column names.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse timestamps\n",
        "raw = safe_to_datetime(raw, COL_TIMESTAMP, tz=TIMEZONE)\n",
        "print('After parsing timestamps:')\n",
        "print_df_info(raw, 'raw')\n",
        "\n",
        "# Drop rows where timestamp or junction is missing \u2014 they cannot be used for aggregation\n",
        "raw = raw.dropna(subset=[COL_TIMESTAMP, COL_JUNCTION])\n",
        "\n",
        "# Ensure counts numeric\n",
        "raw[COL_COUNT] = pd.to_numeric(raw[COL_COUNT], errors='coerce')\n",
        "\n",
        "# Basic duplicates handling\n",
        "pre_dup = len(raw)\n",
        "raw = raw.drop_duplicates()\n",
        "print(f\"Dropped {pre_dup - len(raw)} duplicate rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate to hourly intervals per junction\n",
        "# Step: set timestamp as index for resampling convenience\n",
        "raw = raw.set_index(COL_TIMESTAMP)\n",
        "\n",
        "# If there are other columns (e.g., vehicle_type) you may want to pivot or aggregate them separately.\n",
        "agg = (raw\n",
        "       .groupby([pd.Grouper(freq=RESAMPLE_FREQ), COL_JUNCTION])\n",
        "       [COL_COUNT]\n",
        "       .sum()\n",
        "       .reset_index()\n",
        "       .rename(columns={COL_COUNT: 'vehicle_count_hourly'}))\n",
        "\n",
        "# Ensure continuous hourly grid per junction (fill missing hours with 0 or NaN depending on preference).\n",
        "# We'll create a full grid and left-join to guarantee consistent time rows for each junction.\n",
        "junctions = agg[COL_JUNCTION].unique()\n",
        "full_frames = []\n",
        "for j in junctions:\n",
        "    jdf = agg[agg[COL_JUNCTION] == j].set_index(COL_TIMESTAMP).sort_index()\n",
        "    idx = pd.date_range(start=jdf.index.min(), end=jdf.index.max(), freq=RESAMPLE_FREQ)\n",
        "    jdf = jdf.reindex(idx)\n",
        "    jdf[COL_JUNCTION] = j\n",
        "    jdf.index.name = COL_TIMESTAMP\n",
        "    full_frames.append(jdf.reset_index())\n",
        "\n",
        "agg_full = pd.concat(full_frames, ignore_index=True)\n",
        "# Missing vehicle counts likely mean 0 (no vehicles recorded) or missing sensor -- choose your domain choice.\n",
        "# We'll impute missing hourly counts with 0 (comment: change to .fillna(method='ffill') if sensor gaps)\n",
        "agg_full['vehicle_count_hourly'] = agg_full['vehicle_count_hourly'].fillna(0)\n",
        "print('Aggregated shape (hourly per junction):', agg_full.shape)\n",
        "agg_full.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based features\n",
        "df = agg_full.copy()\n",
        "df[COL_TIMESTAMP] = pd.to_datetime(df[COL_TIMESTAMP])  # ensure column exists\n",
        "df['hour'] = df[COL_TIMESTAMP].dt.hour\n",
        "df['day_of_week'] = df[COL_TIMESTAMP].dt.dayofweek      # Monday=0\n",
        "df['weekday_name'] = df[COL_TIMESTAMP].dt.day_name()\n",
        "df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
        "df['month'] = df[COL_TIMESTAMP].dt.month\n",
        "\n",
        "# cyclical encoding of hour (useful for many models)\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "print_df_info(df, 'df (after time features)')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lag and rolling features\n",
        "def add_lag_features(df, group_col, value_col, lags=[1,2,3,24,48], rolling_windows=[3,6,24]):\n",
        "    df = df.sort_values([group_col, COL_TIMESTAMP]).reset_index(drop=True)\n",
        "    for lag in lags:\n",
        "        col_name = f'{value_col}_lag_{lag}'\n",
        "        df[col_name] = df.groupby(group_col)[value_col].shift(lag)\n",
        "    for w in rolling_windows:\n",
        "        col_name = f'{value_col}_rollmean_{w}'\n",
        "        df[col_name] = df.groupby(group_col)[value_col].shift(1).rolling(w).mean().reset_index(level=0, drop=True)\n",
        "    return df\n",
        "\n",
        "df = add_lag_features(df, COL_JUNCTION, 'vehicle_count_hourly', lags=[1,2,3,24], rolling_windows=[3,6,24])\n",
        "# Replace NaNs from lag features with 0 (or keep NaN if you prefer explicit missingness)\n",
        "lag_cols = [c for c in df.columns if 'lag' in c or 'rollmean' in c]\n",
        "df[lag_cols] = df[lag_cols].fillna(0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge special events (if provided)\n",
        "events_path = os.path.join(RAW_DATA_DIR, EVENTS_FILE) if EVENTS_FILE else None\n",
        "if events_path and os.path.exists(events_path):\n",
        "    events = pd.read_csv(events_path)\n",
        "    # Expected minimal structure: either a single date column or start_date/end_date\n",
        "    # Normalize events to a per-date flag\n",
        "    if 'date' in events.columns:\n",
        "        events['date'] = pd.to_datetime(events['date']).dt.date\n",
        "        event_dates = set(events['date'].tolist())\n",
        "        df['is_event'] = df[COL_TIMESTAMP].dt.date.isin(event_dates).astype(int)\n",
        "    elif 'start_date' in events.columns and 'end_date' in events.columns:\n",
        "        events['start_date'] = pd.to_datetime(events['start_date']).dt.date\n",
        "        events['end_date'] = pd.to_datetime(events['end_date']).dt.date\n",
        "        def check_event(d):\n",
        "            for _, r in events.iterrows():\n",
        "                if r['start_date'] <= d <= r['end_date']:\n",
        "                    return 1\n",
        "            return 0\n",
        "        df['is_event'] = df[COL_TIMESTAMP].dt.date.apply(check_event)\n",
        "    else:\n",
        "        print('events.csv found but format not recognized. Expected `date` or `start_date`/`end_date`.')\n",
        "else:\n",
        "    df['is_event'] = 0\n",
        "    print('No events file found; is_event = 0 for all rows.')\n",
        "df['is_event'] = df['is_event'].fillna(0).astype(int)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple visualizations \u2014 one per plot as recommended\n",
        "\n",
        "# 1) Distribution of hourly vehicle counts (overall)\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.hist(df['vehicle_count_hourly'].clip(upper=df['vehicle_count_hourly'].quantile(0.99)), bins=50)\n",
        "plt.title('Distribution of hourly vehicle counts (capped at 99th pct)')\n",
        "plt.xlabel('vehicle_count_hourly')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()\n",
        "\n",
        "# 2) Time-series for a sample junction (first junction)\n",
        "sample_junc = df[COL_JUNCTION].unique()[0]\n",
        "sample_ts = df[df[COL_JUNCTION] == sample_junc].set_index(COL_TIMESTAMP)['vehicle_count_hourly']\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(sample_ts.index, sample_ts.values)\n",
        "plt.title(f'Hourly vehicle count \u2014 junction {sample_junc}')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('vehicle_count_hourly')\n",
        "plt.show()\n",
        "\n",
        "# 3) Average hourly profile (mean per hour of day)\n",
        "hourly_profile = df.groupby('hour')['vehicle_count_hourly'].mean()\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(hourly_profile.index, hourly_profile.values, marker='o')\n",
        "plt.title('Average traffic profile by hour of day (all junctions)')\n",
        "plt.xlabel('hour (0-23)')\n",
        "plt.ylabel('mean vehicle_count_hourly')\n",
        "plt.xticks(range(0,24))\n",
        "plt.show()\n",
        "\n",
        "# 4) Boxplot of counts by weekday\n",
        "plt.figure(figsize=(10,5))\n",
        "# create list of arrays per weekday for plotting\n",
        "weekday_data = [df[df['day_of_week']==i]['vehicle_count_hourly'].values for i in range(7)]\n",
        "plt.boxplot(weekday_data, labels=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], showfliers=False)\n",
        "plt.title('Hourly counts distribution by weekday (no outliers)')\n",
        "plt.ylabel('vehicle_count_hourly')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling/Normalization example\n",
        "numeric_cols = ['vehicle_count_hourly'] + [c for c in lag_cols if c in df.columns] + ['hour_sin', 'hour_cos']\n",
        "numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
        "\n",
        "print('Scaled numeric columns with StandardScaler. Sample:')\n",
        "df_scaled[numeric_cols].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection: correlation matrix\n",
        "corr_cols = ['vehicle_count_hourly'] + [c for c in df_scaled.columns if c not in [COL_TIMESTAMP, COL_JUNCTION, 'weekday_name']]\n",
        "corr = df_scaled[corr_cols].corr()\n",
        "print('Top correlations with target:')\n",
        "print(corr['vehicle_count_hourly'].abs().sort_values(ascending=False).head(10))\n",
        "\n",
        "# RandomForest feature importance (quick demonstration)\n",
        "X = df_scaled.drop(columns=[COL_TIMESTAMP, COL_JUNCTION, 'vehicle_count_hourly', 'weekday_name'])\n",
        "y = df_scaled['vehicle_count_hourly']\n",
        "# Simple train-test split for importance estimation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.fillna(0), y.fillna(0), test_size=0.2, random_state=42)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "imp = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print('\\nRandomForest importances (top 20):')\n",
        "print(imp.head(20))\n",
        "\n",
        "# Optional: keep top K features\n",
        "top_k = imp.head(20).index.tolist()\n",
        "X_top = X[top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final aggregated & preprocessed CSV for submission\n",
        "out_path = os.path.join('.', 'aggregated_preprocessed.csv')\n",
        "# Choose df or df_scaled as your final output. We'll save both.\n",
        "df.to_csv(out_path.replace('.csv', '_rawfeatures.csv'), index=False)\n",
        "df_scaled.to_csv(out_path.replace('.csv', '_scaled.csv'), index=False)\n",
        "print('Saved files:', out_path.replace('.csv', '_rawfeatures.csv'), 'and', out_path.replace('.csv', '_scaled.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick baseline model RMSE on test set (using top features)\n",
        "y_pred = rf.predict(X_test.fillna(0))\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(f'Baseline RandomForest RMSE (on scaled target): {rmse:.4f}')\n",
        "# Note: target was scaled; if you want RMSE in original units, inverse-transform the target scaling accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps & submission tips\n",
        "\n",
        "- Inspect the saved files `*_rawfeatures.csv` and `*_scaled.csv`. These are ready for model training.  \n",
        "- If your assignment requires a single file under 10 MB, compress the CSV or choose the scaled/raw file as appropriate.  \n",
        "- Update the `CONFIG` cell at the top to match exact column names in your dataset before running.  \n",
        "- For production modeling, consider:\n",
        "  - More carefully handling missing sensor intervals vs true zeros.\n",
        "  - Encoding categorical features (junction metadata) with embeddings or target encoding.\n",
        "  - Using cross-validation over time (time-series split) rather than random train-test split.\n",
        "  - Feature selection using time-series-aware validation and permutation importance.\n",
        "\n",
        "**Deliverables for submission (recommended):**\n",
        "1. `Aggregated_Preprocessed_Data_Karthik.ipynb` \u2014 this notebook with code, comments, and plots.\n",
        "2. `aggregated_preprocessed_rawfeatures.csv` \u2014 aggregated with engineered features.\n",
        "3. `aggregated_preprocessed_scaled.csv` \u2014 scaled numeric features for direct model training.\n",
        "4. Short presentation (PDF/PowerPoint) summarizing EDA findings and peak-hour insights.\n",
        "\n",
        "Good luck \u2014 if you want, I can further tailor the notebook to your exact column names or add specific visualizations (heatmap of correlations, interactive plots) \u2014 tell me your raw CSV header and I'll adapt the notebook automatically.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}